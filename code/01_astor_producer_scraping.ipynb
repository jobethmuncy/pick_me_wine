{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astor Producers\n",
    "Astor is SQL based and will only show up to 100 wines. This must be scraped in a way to make sure there is never more than 100 wines per page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "# from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.astorwines.com/hub.aspx?type=wine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start here: get producers \n",
    "https://www.astorwines.com/hub.aspx?type=wine<br>\n",
    "Dropdown menu of all producers<br>\n",
    "Reformated to fit into a f-string later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1894"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get text from producers list on first drop down page \n",
    "\n",
    "producers = soup.find('select', {'id': 'ctl00_ctl00_middleContent_pageContent_ctl00_ddlProducer'})\n",
    "\n",
    "listings = producers.select('option[value]')\n",
    "values = [listing.get('value') for listing in listings[1:]]\n",
    "\n",
    "strings = values\n",
    "\n",
    "new_strings = []\n",
    "\n",
    "for string in strings:\n",
    "    new_string = string.replace(' ', '+')\n",
    "\n",
    "    new_strings.append(new_string)\n",
    "\n",
    "# this is a list of producers formated to go into the slug \n",
    "len(new_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is not an official rate limit on scraping from this site, if the timer is not high enough or I try to get too many at a time it returns a bad handshake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test on a small section of the producers \n",
    "\n",
    "# first = new_strings[:100]  # 163*\n",
    "# second = new_strings[101:200]  # 184*\n",
    "# third = new_strings[201:300]  # 136*\n",
    "# fourth = new_strings[301:400] # 144*\n",
    "# fifth = new_strings[401:500]  # 132*\n",
    "# sixth = new_strings[501:600]  # 178*\n",
    "# seventh = new_strings[601:700]  # 129 rerun\n",
    "# eighth = new_strings[701:800]  # 162*\n",
    "# nineth = new_strings[801:900]  # 219*\n",
    "# tenth = new_strings[901:1000]  # 187*\n",
    "# eleven = new_strings[1001:1100]  # 144*\n",
    "# twelve = new_strings[1101:1200]  # 168*\n",
    "# thirteen = new_strings[1201:1300]  # 186*\n",
    "# fourteen = new_strings[1301:1400]  # 171*\n",
    "# fifteen = new_strings[1401:1500]  # 192*\n",
    "# sixteen = new_strings[1501:1600]  # 204*\n",
    "# seventeen = new_strings[1601:1700]  # 150*\n",
    "# eighteen = new_strings[1701:1800]  # 177*\n",
    "# nineteen = new_strings[1801:1900]  # 162\n",
    "\n",
    "## 3188 total wines from Astor.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested for-loops \n",
    "First get to https://www.astorwines.com/WineSearchResult.aspx?p=1&search=Advanced&searchtype=Contains&term=&cat=1&producer=2Naturkinder<br>\n",
    "Collect the href from the individual wines<br>\n",
    "Then get to https://www.astorwines.com/SearchResultsSingle.aspx?p=1&search=42380&searchtype=Contains<br><br>\n",
    "Broke up the wines into groups of 100 producers. There is no robots.txt on Astor or API but even with a large time delay between wine, it will still return a bad handshake if the function is running for too long. The function was ran on smaller chunks of the data and pickled for the next step of extracting the details and tasting notes from the individual pages for each wine. I had tried to incooperate that into this loop but even with the delays and Selenium it would not allow for more than a few scrapes before it returned an error.<br><br>\n",
    "I have the pickling commented out as I accidently overrode a file of wine urls so I have it commented out to avoid the same mistake. Also a return wines would show all the urls but I wanted to save them in a seperate location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_names(names):\n",
    "\n",
    "    wines = [] # eventually will add to this \n",
    "\n",
    "    for name in names:\n",
    "        print('SCRAPING {}'.format(name))\n",
    "\n",
    "    #     tag = name\n",
    "        wine_url = f'https://www.astorwines.com/WineSearchResult.aspx?p=1&search=Advanced&searchtype=Contains&term=&cat=1&producer={name}'\n",
    "\n",
    "        wine_res = requests.get(wine_url)\n",
    "\n",
    "        wine_soup = BeautifulSoup(wine_res.content, 'lxml')\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        for listing in wine_soup.find_all('h2'):\n",
    "\n",
    "            href = listing.find('a')['href']\n",
    "            page_url = f'https://www.astorwines.com/{href}'\n",
    "\n",
    "            time.sleep(5)\n",
    "\n",
    "            wines.append(page_url)\n",
    "            \n",
    "            #### COMMENT OUT WHEN NOT USING ####\n",
    "\n",
    "#             with open('601-700.txt', 'wb') as fp: \n",
    "#                 pickle.dump(wines, fp)\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seventh = new_strings[601:700] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRAPING de+Trafford\n",
      "SCRAPING De+Vescovi-Ulzbach\n",
      "SCRAPING De+Wetshof\n",
      "SCRAPING Defaix\n",
      "SCRAPING Delacroix\n",
      "SCRAPING Delas\n",
      "SCRAPING Delinquente+Wine+Co.+\n",
      "SCRAPING Delouvin-Nowack\n",
      "SCRAPING Delsignore\n",
      "SCRAPING Demière-Ansiot\n",
      "SCRAPING Demougeot\n",
      "SCRAPING Denis+Jamain\n",
      "SCRAPING Dénis+Meunier\n",
      "SCRAPING Denis+Mortet\n",
      "SCRAPING Denny+Bini-Podere+Cipolla\n",
      "SCRAPING Depeuble\n",
      "SCRAPING Descendentes+de+José+Palacios\n",
      "SCRAPING Deschamps\n",
      "SCRAPING Desvignes\n",
      "SCRAPING Devevey\n",
      "SCRAPING Di+Filippo\n",
      "SCRAPING Die+Hochland+Imker\n",
      "SCRAPING Diemersdal\n",
      "SCRAPING Dios+Baco\n",
      "SCRAPING Dirler\n",
      "SCRAPING Dirty+&+Rowdy\n",
      "SCRAPING Dirupi\n",
      "SCRAPING Dogliani\n",
      "SCRAPING Dom+de+la+Romanee-Conti\n",
      "SCRAPING Dom+Pérignon\n",
      "SCRAPING Dom.+A&P+de+Villaine\n",
      "SCRAPING Dom.+André+&+Mireille+Tissot\n",
      "SCRAPING Dom.+André+Clouet\n",
      "SCRAPING Dom.+Aux+Moines\n",
      "SCRAPING Dom.+Bart\n",
      "SCRAPING Dom.+Bernard+Ange\n",
      "SCRAPING Dom.+Bersan\n",
      "SCRAPING Dom.+Bois+de+Boursan\n",
      "SCRAPING Dom.+Carneros\n",
      "SCRAPING Dom.+Chante+Cigale\n",
      "SCRAPING Dom.+Chevrot\n",
      "SCRAPING Dom.+Chofflet-Valdenaire\n",
      "SCRAPING Dom.+Clape\n",
      "SCRAPING Dom.+d'Ardhuy\n",
      "SCRAPING Dom.+de+Champarlan\n",
      "SCRAPING Dom.+de+Chevalier\n",
      "SCRAPING Dom.+de+Fages\n",
      "SCRAPING Dom.+De+Fosse-Sèche\n",
      "SCRAPING Dom.+de+l'Ecu\n",
      "SCRAPING Dom.+de+l'Enclos\n",
      "SCRAPING Dom.+de+l'Oubliée\n",
      "SCRAPING Dom.+De+l'R\n",
      "SCRAPING Dom.+De+la+Bastide\n",
      "SCRAPING Dom.+de+la+Bergerie\n",
      "SCRAPING Dom.+de+la+Coudette\n",
      "SCRAPING Dom.+de+la+Grosse+Pierre\n",
      "SCRAPING Dom.+de+la+Haute+Olive\n",
      "SCRAPING Dom.+de+la+Navicelle\n",
      "SCRAPING Dom.+de+la+Patience\n",
      "SCRAPING Dom.+de+la+Rectorie+\n",
      "SCRAPING Dom.+de+la+Roche\n",
      "SCRAPING Dom.+de+la+Romanée-Conti\n",
      "SCRAPING Dom.+de+la+Selve\n",
      "SCRAPING Dom.+De+la+Tour+du+Bon\n",
      "SCRAPING Dom.+De+la+Voute+des+Crozes\n",
      "SCRAPING Dom.+de+Monbourgeau\n",
      "SCRAPING Dom.+de+Montcy\n",
      "SCRAPING Dom.+de+Montille\n",
      "SCRAPING Dom.+De+Montrieux\n",
      "SCRAPING Dom.+de+Montvac\n",
      "SCRAPING Dom.+de+Périllière\n",
      "SCRAPING Dom.+de+Roally\n",
      "SCRAPING Dom.+de+Sarrelon\n",
      "SCRAPING Dom.+des+Amiel\n",
      "SCRAPING Dom.+Des+Billards\n",
      "SCRAPING Dom.+Des+Cassagnoles\n",
      "SCRAPING Dom.+des+Cognettes\n",
      "SCRAPING Dom.+des+Crêts\n",
      "SCRAPING Dom.+des+Deux+Anes\n",
      "SCRAPING Dom.+des+Deux+Ânes\n",
      "SCRAPING Dom.+des+Geneves\n",
      "SCRAPING Dom.+des+Meix+Poron\n",
      "SCRAPING Dom.+des+Rouges-Queues\n",
      "SCRAPING Dom.+des+Tourelles\n",
      "SCRAPING Dom.+Didier+Dagueneau\n",
      "SCRAPING Dom.+du++Moulin\n",
      "SCRAPING Dom.+Du+Bagnol\n",
      "SCRAPING Dom.+Du+Bel+Air\n",
      "SCRAPING Dom.+Du+Castel\n",
      "SCRAPING Dom.+du+Cellier+des+Cray\n",
      "SCRAPING Dom.+Du+Closel\n",
      "SCRAPING Dom.+Du+Cros\n",
      "SCRAPING Dom.+Du+Grapillon+d'Or\n",
      "SCRAPING Dom.+du+Gros'+Noré\n",
      "SCRAPING Dom.+du+Haut+Bourg\n",
      "SCRAPING Dom.+Du+Jaugaret\n",
      "SCRAPING Dom.+du+Pech\n",
      "SCRAPING Dom.+du+Salvard\n",
      "SCRAPING Dom.+du+Viking\n"
     ]
    }
   ],
   "source": [
    "get_names(seventh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.astorwines.com/SearchResultsSingle.aspx?p=1&search=42380&searchtype=Contains',\n",
       " 'https://www.astorwines.com/SearchResultsSingle.aspx?p=1&search=37134&searchtype=Contains',\n",
       " 'https://www.astorwines.com/SearchResultsSingle.aspx?p=1&search=23783&searchtype=Contains',\n",
       " 'https://www.astorwines.com/SearchResultsSingle.aspx?p=1&search=24189&searchtype=Contains',\n",
       " 'https://www.astorwines.com/SearchResultsSingle.aspx?p=1&search=32294&searchtype=Contains',\n",
       " 'https://www.astorwines.com/SearchResultsSingle.aspx?p=1&search=21913&searchtype=Contains']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('test.txt', 'rb') as fp:\n",
    "    b = pickle.load(fp)\n",
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
